import os
import requests
import xml.etree.ElementTree as ET
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document

def fetch_api_docs(service_key: str, max_page: int = 1) -> list[Document]:
    """
    ì—¬ì„±ê°€ì¡±ë¶€ ì•„ì´ëŒë³´ë¯¸ ë³´ìˆ˜êµìœ¡ API í˜¸ì¶œ â†’ XML íŒŒì‹± â†’ LangChain Documentë¡œ ë³€í™˜
    """
    docs = []
    for page in range(1, max_page + 1):
        url = (
            "http://apis.data.go.kr/1383000/idis/refresherEducationService/getRefresherEducationList"
            f"?serviceKey={service_key}"
            f"&pageNo={page}&numOfRows=10&type=xml"
        )

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            root = ET.fromstring(response.content)

            for item in root.iter("item"):
                crtrYr = item.findtext("crtrYr", default="N/A")
                eduDvsnNm = item.findtext("eduDvsnNm", default="N/A")
                eduCrsNm = item.findtext("eduCrsNm", default="N/A")
                eduCn = item.findtext("eduCn", default="N/A")
                date = item.findtext("dataCrtrYmd", default="N/A")

                text = (
                    f"[{date}] ê¸°ì¤€ì—°ë„: {crtrYr}, êµìœ¡êµ¬ë¶„: {eduDvsnNm}, "
                    f"ê³¼ì •ëª…: {eduCrsNm}, ë‚´ìš©: {eduCn}"
                )
                docs.append(Document(page_content=text, metadata={"source": f"RefresherEdu_{date}"}))

        except Exception as e:
            print(f"âŒ API í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")

    print(f"ğŸŒ ë³´ìˆ˜êµìœ¡ API ë¬¸ì„œ {len(docs)}ê°œ ë¡œë“œ ì™„ë£Œ")
    return docs

def prepare_chroma_db_all(data_dir: str, persist_dir: str, service_key: str):
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ + API XML ë°ì´í„° â†’ ë²¡í„° DB ì €ì¥
    """
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    all_documents = []

    # 1. í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¡œë“œ
    for root, _, files in os.walk(data_dir):
        for file in files:
            if not file.endswith(".txt"):
                continue
            file_path = os.path.join(root, file)
            try:
                loader = TextLoader(file_path, encoding="utf-8")
                text = loader.load()[0].page_content
                doc = Document(page_content=text, metadata={"source": file_path})
                all_documents.append(doc)
                print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {file_path}")
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì—ëŸ¬: {file_path} - {str(e)}")

    # 2. API XML ë¡œë“œ
    api_docs = fetch_api_docs(service_key)
    all_documents.extend(api_docs)

    # 3. Chroma DB ì €ì¥
    vector_db = Chroma.from_documents(
        documents=all_documents,
        embedding=embedding_model,
        persist_directory=persist_dir
    )
    print(f"ğŸ‰ ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ë²¡í„° ì €ì¥ ì™„ë£Œ!")
    
    # ì €ì¥ëœ ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°
    vector_db = Chroma(
        embedding_function=embedding_model,
        persist_directory=persist_dir
    )

    query = "ì•„ë™ ì‹¬ë¦¬ êµìœ¡ì€ ì–´ë–¤ ë‚´ìš©ì„ í¬í•¨í•˜ë‚˜ìš”?"
    docs = vector_db.similarity_search(query, k=3)

    for i, doc in enumerate(docs, 1):
        print(f"\nğŸ” [{i}ìœ„ ê²°ê³¼]")
        print(doc.page_content)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§ª ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    data_dir = "./data"
    persist_dir = "./vectorDB"

    # ğŸ“Œ Encodingëœ ì„œë¹„ìŠ¤ í‚¤ ì‚¬ìš©!
    service_key = "gGxYEcfSjjqDpdu6Amo9r9uY%2FPL9yiXpWYpu1%2BPGbk9sKfjAdwP%2FjXzfI0d%2BEB%2B80Ew4Wd4AB%2FiMd1hjpWy9SA%3D%3D"

    prepare_chroma_db_all(data_dir, persist_dir, service_key)
